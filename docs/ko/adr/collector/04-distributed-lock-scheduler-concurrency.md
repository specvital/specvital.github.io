---
title: 분산 락 기반 스케줄러 동시성 제어
---

# ADR-04: 분산 락 기반 스케줄러 동시성 제어

> 🇺🇸 [English Version](/en/adr/collector/04-distributed-lock-scheduler-concurrency.md)

| 날짜       | 작성자       | 리포지토리 |
| ---------- | ------------ | ---------- |
| 2024-12-18 | @KubrickCode | collector  |

## 컨텍스트

### 다중 인스턴스 문제

Blue-Green 배포 및 수평 확장 환경에서 전환 중 여러 스케줄러 인스턴스가 동시에 실행될 수 있음:

```
Blue-Green 배포:
[Blue v1.0]  ────────────────────────────────►  [종료]
                    ↑ 중첩 기간 ↓
             [Green v1.1]  ────────────────────────────────►

중첩 기간: 두 인스턴스가 스케줄 작업 시도 → 중복 실행
```

**조율 없이:**

- 동일 저장소 중복 분석
- 워커 큐에 중복 작업 추가
- 컴퓨팅 리소스 낭비
- 잠재적 데이터 불일치

### 요구사항

1. **단일 실행 보장**: 특정 시점에 하나의 스케줄러 인스턴스만 스케줄 작업 실행
2. **자동 복구**: 활성 인스턴스 크래시 시 다른 인스턴스가 인계
3. **수동 개입 불필요**: 죽은 인스턴스에 대한 운영자 조치 불필요
4. **최소 인프라**: 기존 스택 외 추가 의존성 회피

## 결정

**UUID 토큰 소유권과 하트비트 기반 리스 연장을 가진 Redis 기반 분산 락 구현.**

핵심 컴포넌트:

1. **SETNX 기반 락 획득**: 원자적 비교-설정으로 락 생성
2. **UUID 토큰 소유권**: 만료된 보유자의 잘못된 해제 방지
3. **Lua 스크립트 원자성**: 검사-수정 연산의 race condition 방지
4. **하트비트 리스 연장**: 초기 TTL을 초과하는 장시간 작업 지원

## 검토한 대안

### 옵션 A: Redis SETNX + Token (선택됨)

**메커니즘:**

```
획득: SETNX key=lockKey value=UUID TTL=10min
연장: Lua(if GET(key)==UUID then EXPIRE(key, TTL))
해제: Lua(if GET(key)==UUID then DEL(key))
```

**장점:**

- 단일 의존성 (Redis는 이미 Asynq 큐에서 사용)
- 밀리초 수준 성능
- 토큰 검증으로 실수로 인한 해제 방지
- 자동 만료로 데드락 방지
- 클럭 동기화 불필요

**단점:**

- 단일 Redis 인스턴스는 단일 장애점
- 신중한 TTL/하트비트 튜닝 필요

### 옵션 B: Zookeeper/etcd 합의

**메커니즘:**

- 리더 선출을 위한 임시 순차 노드 생성
- 노드 변경 감시로 리더 실패 감지

**장점:**

- 강한 일관성 보장
- 내장 리더 선출 프리미티브
- 네트워크 파티션 적절히 처리

**단점:**

- 추가 인프라 (별도 클러스터 필요)
- 운영 복잡도 (쿼럼 유지)
- 단일 스케줄러 사용 사례에 과도함
- Redis보다 높은 지연시간

### 옵션 C: 데이터베이스 Advisory Lock

**메커니즘:**

- PostgreSQL `pg_advisory_lock` 또는 `SELECT FOR UPDATE`
- 주기적 폴링으로 락 확인/획득

**장점:**

- 기존 PostgreSQL 인프라 활용
- 트랜잭션 보장

**단점:**

- 폴링이 데이터베이스 리소스 낭비
- 장시간 보유 락에 적합하지 않음
- 데드락 시나리오 가능성
- 연결 종속적 (연결 끊김 시 손실)

## 구현 원칙

### 토큰 기반 소유권

핵심 통찰: 작업 완료 후 단순 `DEL key`는 안전하지 않음.

**문제 시나리오:**

```
[0:00] 인스턴스 A 락 획득 (TTL=10min)
[0:10] TTL 만료 (A가 긴 GC 일시 정지)
[0:11] 인스턴스 B 락 획득
[0:12] 인스턴스 A 재개, Release 호출
[0:12] 인스턴스 A가 인스턴스 B의 락 삭제 ← 버그
```

**해결책:**

```go
// 획득: 고유 토큰을 값으로 저장
token := uuid.New().String()
ok := redis.SetNX(key, token, ttl)
if ok {
    self.token = token  // 소유권 기록
}

// 해제: 여전히 소유자인 경우에만 삭제
if redis.Get(key) == self.token {
    redis.Del(key)
}
```

### Lua 스크립트를 통한 원자적 연산

소유권 확인과 상태 수정은 원자적이어야 함. Redis Lua 스크립트는 원자적으로 실행:

```lua
-- 해제: 조건부 삭제
if redis.call("get", KEYS[1]) == ARGV[1] then
    return redis.call("del", KEYS[1])
else
    return 0
end

-- 연장: 조건부 TTL 갱신
if redis.call("get", KEYS[1]) == ARGV[1] then
    return redis.call("expire", KEYS[1], ARGV[2])
else
    return 0
end
```

**왜 별도 GET + DEL이 아닌가?**

```
[T1] 인스턴스 A: GET key → "token-a" (일치)
[T2] 인스턴스 A: (컨텍스트 스위치 / 네트워크 지연)
[T3] TTL 만료, 인스턴스 B가 "token-b"로 획득
[T4] 인스턴스 A: DEL key → B의 락 삭제 ← 버그
```

### 하트비트 전략

초기 TTL을 초과할 수 있는 작업을 위해 백그라운드 하트비트가 리스 연장:

```
설정:
├── Lock TTL: 10분
├── 하트비트 간격: 3분
└── 작업 타임아웃: 5분 (소프트 리밋)

타임라인 (정상 케이스):
[0:00] 락 획득, TTL=10min, 하트비트 고루틴 시작
[0:03] 하트비트 → TTL을 현재부터 10min으로 연장
[0:05] 작업 완료, 락 해제
[0:05] 하트비트 고루틴 종료

타임라인 (긴 작업):
[0:00] 락 획득
[0:03] 하트비트 → 연장
[0:06] 하트비트 → 연장
[0:08] 작업 완료, 락 해제

타임라인 (크래시 복구):
[0:00] 인스턴스 A 락 획득
[0:03] 인스턴스 A 크래시 (더 이상 하트비트 없음)
[0:10] TTL 만료, 락 자동 해제
[0:10] 인스턴스 B 획득 가능
```

**설계 근거:**

- `하트비트 < TTL/3`: 만료 전 최소 2회 연장 시도 보장
- `TTL > 작업 타임아웃`: 정상 작업은 만료 위험 전 완료
- 하트비트는 작업 실행 중에만 동작 (스케줄 실행 사이에는 비활성)

### Graceful Degradation

락 획득 실패 시 (Redis 불가), 핸들러는 로그 기록 후 건너뜀:

```go
acquired, err := lock.TryAcquire(ctx)
if err != nil {
    slog.Error("lock acquisition failed", "error", err)
    return  // 이번 사이클 건너뜀, 다음 스케줄에 재시도
}
if !acquired {
    slog.Debug("skipped: another instance is running")
    return  // 중첩 기간 중 정상 케이스
}
```

**동작:**

- Redis 다운: 모든 인스턴스 건너뜀 (Redis 복구 전까지 작업 미실행)
- 다른 인스턴스가 락 보유 중: 인스턴스는 조용히 건너뜀
- 둘 다 INFO/DEBUG 레벨 (예상되는 상황이므로 ERROR 아님)

## 결과

### 긍정적

**Blue-Green 배포 안전성:**

- 롤링 배포 중 하나의 인스턴스만 스케줄 작업 실행
- 전환 기간 중 중복 작업 추가 없음
- 무중단 배포 안전성 유지

**자동 장애 복구:**

- 활성 스케줄러 크래시 시 TTL 후 락 만료
- 대기 인스턴스가 자동으로 인계
- 수동 개입이나 모니터링 알림 불필요

**리소스 효율성:**

- 기존 Redis 연결 재사용 (Asynq와 공유)
- 추가 인프라 프로비저닝 불필요
- 락 연산에 최소한의 메모리/CPU 오버헤드

**관측성:**

- 적절한 레벨에서 락 획득/해제 로깅
- 로그를 통해 어느 인스턴스가 활성인지 쉽게 추적
- 필요 시 Redis CLI로 락 상태 검사 가능

### 부정적

**Redis 의존성:**

- Redis 불가 시 모든 스케줄 작업 실행 차단
- 단일 장애점 (Redis HA 미구성 시)
- 스케줄러가 "어쨌든 실행" 모드로 폴백 불가

**시간 기반 보장:**

- 극단적 GC 일시 정지 (>TTL) 시 이론적으로 이중 실행 가능
- 하트비트로 완화되나 수학적으로 불가능하지 않음
- 합의 프로토콜 대비 단순성을 위한 수용 가능한 트레이드오프

**디버깅 복잡도:**

- 분산 락 문제 로컬 재현 어려움
- 락 상태 검사를 위해 Redis 접근 필요
- 타이밍 의존 버그는 간헐적일 수 있음

### 기술 사양

| 파라미터      | 값                            | 근거                                |
| ------------- | ----------------------------- | ----------------------------------- |
| Lock Key      | `scheduler:auto-refresh:lock` | 충돌 방지를 위한 네임스페이스       |
| TTL           | 10분                          | > 작업 타임아웃, 하트비트 복구 허용 |
| 하트비트 간격 | 3분                           | < TTL/3 안전 마진                   |
| 토큰 형식     | UUID v4                       | 전역 고유, 조율 불필요              |

## 참고자료

- [ADR-01: 스케줄 기반 재수집](./01-scheduled-recollection.md) (이 락 메커니즘 사용)
- [ADR-03: Graceful Shutdown](./03-graceful-shutdown.md) (종료 시 락 해제)
- [ADR-07: Worker-Scheduler 분리](./07-worker-scheduler-separation.md) (스케줄러에 락이 필요한 이유)
- [Redis SETNX 문서](https://redis.io/commands/setnx/)
- [Redlock 알고리즘 논의](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)
